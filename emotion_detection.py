# -*- coding: utf-8 -*-
"""Emotion detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JYG_X25OHiPknMSyuW2hC-7pFvG9agES
"""

!wget https://github.com/oarriaga/face_classification/raw/master/trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5 -O emotion_model.h5







from google.colab import files
uploaded = files.upload()







# âœ… Install dependencies
!pip install -q keras opencv-python

# âœ… Download pre-trained emotion model (Mini-XCEPTION, expects 64x64 grayscale)
!wget -q https://github.com/oarriaga/face_classification/raw/master/trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5 -O emotion_model.h5

# âœ… Import everything
import cv2
import numpy as np
from keras.models import load_model
from google.colab import files
from google.colab.patches import cv2_imshow

# âœ… Load model
model = load_model("emotion_model.h5", compile=False)
emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']

# âœ… Upload image
uploaded = files.upload()

# âœ… Load face detector
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")

# âœ… Process uploaded image
for filename in uploaded.keys():
    img = cv2.imread(filename)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)

    for (x, y, w, h) in faces:
        roi = gray[y:y+h, x:x+w]
        roi = cv2.resize(roi, (64, 64))  # ðŸ”¥ fixed shape
        roi = roi.astype("float32") / 255.0
        roi = np.expand_dims(roi, axis=-1)  # (64, 64, 1)
        roi = np.expand_dims(roi, axis=0)   # (1, 64, 64, 1)

        preds = model.predict(roi, verbose=0)
        label = emotion_labels[np.argmax(preds)]

        cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)
        cv2.putText(img, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

    # âœ… Show result
    cv2_imshow(img)

# ----------------------- Fix Dependencies -----------------------
!pip install -q openai-whisper librosa==0.10.0.post2 scikit-learn numpy==1.23.5 pydub ffmpeg-python

# ----------------------- Imports -----------------------
import whisper
import librosa
import numpy as np
import os
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from pydub import AudioSegment
from google.colab import files

# ----------------------- Load Whisper Model -----------------------
model = whisper.load_model("base")

# ----------------------- Transcribe Audio -----------------------
def transcribe_audio(audio_path):
    result = model.transcribe(audio_path)
    print("Transcription:", result["text"])
    return result["text"], result["language"]

# ----------------------- Extract Features -----------------------
def extract_audio_features(audio_path):
    y, sr = librosa.load(audio_path, sr=16000)
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
    pitch, _ = librosa.piptrack(y=y, sr=sr)
    pitch_mean = np.mean(pitch, axis=1)

    mfcc_features = np.mean(mfcc, axis=1)
    pitch_features = pitch_mean[:13]

    return np.concatenate([mfcc_features, pitch_features])

# ----------------------- Dummy Emotion Classifier -----------------------
def train_emotion_classifier():
    X = np.random.rand(100, 26)
    y = np.random.choice(['happy', 'sad', 'angry', 'neutral'], size=100)

    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    clf = SVC(kernel='linear')
    clf.fit(X, y_encoded)
    return clf, le

# ----------------------- Classify Emotion -----------------------
def classify_emotion(features, clf, le):
    idx = clf.predict([features])[0]
    return le.inverse_transform([idx])[0]

# ----------------------- Convert to WAV Mono -----------------------
def convert_to_wav(input_path, output_path):
    sound = AudioSegment.from_file(input_path)
    sound = sound.set_channels(1)
    sound = sound.set_frame_rate(16000)
    sound.export(output_path, format="wav")

# ----------------------- Full Pipeline -----------------------
def emotion_aware_speech_recognition(audio_path):
    wav_path = "/content/temp.wav"
    convert_to_wav(audio_path, wav_path)

    transcription, language = transcribe_audio(wav_path)
    features = extract_audio_features(wav_path)
    emotion = classify_emotion(features, emotion_classifier, label_encoder)

    print(f"Detected Emotion: {emotion}")
    print(f"Language Detected: {language}")

    os.remove(wav_path)

# ----------------------- Train Classifier Once -----------------------
emotion_classifier, label_encoder = train_emotion_classifier()

# ----------------------- Upload and Process -----------------------
uploaded = files.upload()

for filename in uploaded.keys():
    path = f"/content/{filename}"
    print(f"\nProcessing file: {path}")
    emotion_aware_speech_recognition(path)